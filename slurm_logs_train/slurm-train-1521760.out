Starting training:
  Algorithm       : a2c
  Environments    : 4
  Reward mode     : temperature
  Observation     : T02
  Seed            : 42
  Timesteps       : 1000000

=== System Info ===
Hostname              : haicn1701.localdomain
CUDA_VISIBLE_DEVICES : MIG-b3ebb06b-a5bd-58d0-9f58-1401d927977b

=== GPU Info (nvidia-smi) ===
Sun Jun  8 15:55:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:31:00.0 Off |                   On |
| N/A   46C    P0             43W /  400W |     249MiB /  40960MiB |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| MIG devices:                                                                            |
+------------------+----------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|        Shared         |
|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |
|                  |                                  |        ECC|                       |
|==================+==================================+===========+=======================|
|  0    1   0   0  |             142MiB / 20096MiB    | 56      0 |  4   0    2    0    0 |
|                  |                 0MiB / 32767MiB  |           |                       |
+------------------+----------------------------------+-----------+-----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

=== PyTorch CUDA Info ===
CUDA available        : True
Device name (ID 0)    : NVIDIA A100-SXM4-40GB MIG 4g.20gb
CUDA version (torch)  : 12.4
CUDNN version         : 90501
Using cuda device
Eval num_timesteps=20000, episode_reward=141.56 +/- 37.23
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 20000    |
| train/                |          |
|    entropy_loss       | -2.07    |
|    explained_variance | -0.0343  |
|    learning_rate      | 0.0003   |
|    n_updates          | 17       |
|    policy_loss        | 59.8     |
|    std                | 1        |
|    value_loss         | 1.3e+03  |
------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=190.39 +/- 29.11
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 190      |
| time/                 |          |
|    total_timesteps    | 40000    |
| train/                |          |
|    entropy_loss       | -3.14    |
|    explained_variance | 0.0721   |
|    learning_rate      | 0.0003   |
|    n_updates          | 34       |
|    policy_loss        | 6.5      |
|    std                | 0.998    |
|    value_loss         | 80       |
------------------------------------
New best mean reward!
Eval num_timesteps=60000, episode_reward=224.70 +/- 19.55
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 225      |
| time/                 |          |
|    total_timesteps    | 60000    |
| train/                |          |
|    entropy_loss       | -3.13    |
|    explained_variance | 0.0803   |
|    learning_rate      | 0.0003   |
|    n_updates          | 52       |
|    policy_loss        | 18.9     |
|    std                | 0.995    |
|    value_loss         | 305      |
------------------------------------
New best mean reward!
Eval num_timesteps=80000, episode_reward=243.22 +/- 12.35
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 243      |
| time/                 |          |
|    total_timesteps    | 80000    |
| train/                |          |
|    entropy_loss       | -2.84    |
|    explained_variance | 0.0839   |
|    learning_rate      | 0.0003   |
|    n_updates          | 69       |
|    policy_loss        | 30.6     |
|    std                | 0.998    |
|    value_loss         | 784      |
------------------------------------
New best mean reward!
Eval num_timesteps=100000, episode_reward=173.42 +/- 9.12
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 173      |
| time/                 |          |
|    total_timesteps    | 100000   |
| train/                |          |
|    entropy_loss       | -2.95    |
|    explained_variance | 0.126    |
|    learning_rate      | 0.0003   |
|    n_updates          | 86       |
|    policy_loss        | 30.6     |
|    std                | 0.999    |
|    value_loss         | 569      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 89.9     |
| time/                 |          |
|    fps                | 801      |
|    iterations         | 100      |
|    time_elapsed       | 143      |
|    total_timesteps    | 115200   |
| train/                |          |
|    entropy_loss       | -2.46    |
|    explained_variance | 0.0629   |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | 50.7     |
|    std                | 0.998    |
|    value_loss         | 1.34e+03 |
------------------------------------
Eval num_timesteps=120000, episode_reward=164.47 +/- 11.39
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 164      |
| time/                 |          |
|    total_timesteps    | 120000   |
| train/                |          |
|    entropy_loss       | -2.8     |
|    explained_variance | 0.0908   |
|    learning_rate      | 0.0003   |
|    n_updates          | 104      |
|    policy_loss        | 47       |
|    std                | 0.997    |
|    value_loss         | 710      |
------------------------------------
Eval num_timesteps=140000, episode_reward=169.92 +/- 12.20
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 170      |
| time/                 |          |
|    total_timesteps    | 140000   |
| train/                |          |
|    entropy_loss       | -3.2     |
|    explained_variance | 0.164    |
|    learning_rate      | 0.0003   |
|    n_updates          | 121      |
|    policy_loss        | 29.5     |
|    std                | 0.996    |
|    value_loss         | 289      |
------------------------------------
Eval num_timesteps=160000, episode_reward=167.55 +/- 10.59
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 168      |
| time/                 |          |
|    total_timesteps    | 160000   |
| train/                |          |
|    entropy_loss       | -2.8     |
|    explained_variance | 0.063    |
|    learning_rate      | 0.0003   |
|    n_updates          | 138      |
|    policy_loss        | 46.9     |
|    std                | 0.995    |
|    value_loss         | 808      |
------------------------------------
Eval num_timesteps=180000, episode_reward=163.33 +/- 7.17
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 163      |
| time/                 |          |
|    total_timesteps    | 180000   |
| train/                |          |
|    entropy_loss       | -2.87    |
|    explained_variance | 0.16     |
|    learning_rate      | 0.0003   |
|    n_updates          | 156      |
|    policy_loss        | 61.5     |
|    std                | 0.997    |
|    value_loss         | 745      |
------------------------------------
Eval num_timesteps=200000, episode_reward=160.08 +/- 6.79
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 160      |
| time/                 |          |
|    total_timesteps    | 200000   |
| train/                |          |
|    entropy_loss       | -3.26    |
|    explained_variance | 0.201    |
|    learning_rate      | 0.0003   |
|    n_updates          | 173      |
|    policy_loss        | 14.9     |
|    std                | 0.995    |
|    value_loss         | 336      |
------------------------------------
Eval num_timesteps=220000, episode_reward=162.08 +/- 6.89
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 162      |
| time/                 |          |
|    total_timesteps    | 220000   |
| train/                |          |
|    entropy_loss       | -3.11    |
|    explained_variance | 0.28     |
|    learning_rate      | 0.0003   |
|    n_updates          | 190      |
|    policy_loss        | 31.6     |
|    std                | 0.993    |
|    value_loss         | 484      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 102      |
| time/                 |          |
|    fps                | 793      |
|    iterations         | 200      |
|    time_elapsed       | 290      |
|    total_timesteps    | 230400   |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | 0.272    |
|    learning_rate      | 0.0003   |
|    n_updates          | 199      |
|    policy_loss        | 33       |
|    std                | 0.992    |
|    value_loss         | 525      |
------------------------------------
Eval num_timesteps=240000, episode_reward=158.28 +/- 4.83
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 158      |
| time/                 |          |
|    total_timesteps    | 240000   |
| train/                |          |
|    entropy_loss       | -2.94    |
|    explained_variance | 0.21     |
|    learning_rate      | 0.0003   |
|    n_updates          | 208      |
|    policy_loss        | 49.6     |
|    std                | 0.993    |
|    value_loss         | 738      |
------------------------------------
Eval num_timesteps=260000, episode_reward=158.21 +/- 2.94
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 158      |
| time/                 |          |
|    total_timesteps    | 260000   |
| train/                |          |
|    entropy_loss       | -2.91    |
|    explained_variance | 0.229    |
|    learning_rate      | 0.0003   |
|    n_updates          | 225      |
|    policy_loss        | 49.8     |
|    std                | 0.991    |
|    value_loss         | 647      |
------------------------------------
Eval num_timesteps=280000, episode_reward=155.79 +/- 2.54
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 156      |
| time/                 |          |
|    total_timesteps    | 280000   |
| train/                |          |
|    entropy_loss       | -2.77    |
|    explained_variance | -0.164   |
|    learning_rate      | 0.0003   |
|    n_updates          | 243      |
|    policy_loss        | 69.4     |
|    std                | 0.989    |
|    value_loss         | 822      |
------------------------------------
Eval num_timesteps=300000, episode_reward=152.11 +/- 4.13
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 152      |
| time/                 |          |
|    total_timesteps    | 300000   |
| train/                |          |
|    entropy_loss       | -3.11    |
|    explained_variance | 0.337    |
|    learning_rate      | 0.0003   |
|    n_updates          | 260      |
|    policy_loss        | 31.2     |
|    std                | 0.987    |
|    value_loss         | 464      |
------------------------------------
Eval num_timesteps=320000, episode_reward=152.63 +/- 2.98
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 153      |
| time/                 |          |
|    total_timesteps    | 320000   |
| train/                |          |
|    entropy_loss       | -2.98    |
|    explained_variance | 0.276    |
|    learning_rate      | 0.0003   |
|    n_updates          | 277      |
|    policy_loss        | 46.3     |
|    std                | 0.987    |
|    value_loss         | 593      |
------------------------------------
Eval num_timesteps=340000, episode_reward=150.54 +/- 1.91
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 151      |
| time/                 |          |
|    total_timesteps    | 340000   |
| train/                |          |
|    entropy_loss       | -3.05    |
|    explained_variance | 0.321    |
|    learning_rate      | 0.0003   |
|    n_updates          | 295      |
|    policy_loss        | 42.9     |
|    std                | 0.985    |
|    value_loss         | 553      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 99.6     |
| time/                 |          |
|    fps                | 791      |
|    iterations         | 300      |
|    time_elapsed       | 436      |
|    total_timesteps    | 345600   |
| train/                |          |
|    entropy_loss       | -2.93    |
|    explained_variance | 0.226    |
|    learning_rate      | 0.0003   |
|    n_updates          | 299      |
|    policy_loss        | 40.1     |
|    std                | 0.985    |
|    value_loss         | 529      |
------------------------------------
Eval num_timesteps=360000, episode_reward=150.36 +/- 1.86
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 150      |
| time/                 |          |
|    total_timesteps    | 360000   |
| train/                |          |
|    entropy_loss       | -2.74    |
|    explained_variance | -0.083   |
|    learning_rate      | 0.0003   |
|    n_updates          | 312      |
|    policy_loss        | 60.9     |
|    std                | 0.984    |
|    value_loss         | 1.04e+03 |
------------------------------------
Eval num_timesteps=380000, episode_reward=149.56 +/- 3.14
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 150      |
| time/                 |          |
|    total_timesteps    | 380000   |
| train/                |          |
|    entropy_loss       | -2.93    |
|    explained_variance | -0.125   |
|    learning_rate      | 0.0003   |
|    n_updates          | 329      |
|    policy_loss        | 65.8     |
|    std                | 0.981    |
|    value_loss         | 632      |
------------------------------------
Eval num_timesteps=400000, episode_reward=148.59 +/- 3.14
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 149      |
| time/                 |          |
|    total_timesteps    | 400000   |
| train/                |          |
|    entropy_loss       | -2.9     |
|    explained_variance | -0.113   |
|    learning_rate      | 0.0003   |
|    n_updates          | 347      |
|    policy_loss        | 51       |
|    std                | 0.98     |
|    value_loss         | 612      |
------------------------------------
Eval num_timesteps=420000, episode_reward=147.16 +/- 3.27
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 147      |
| time/                 |          |
|    total_timesteps    | 420000   |
| train/                |          |
|    entropy_loss       | -3.2     |
|    explained_variance | 0.472    |
|    learning_rate      | 0.0003   |
|    n_updates          | 364      |
|    policy_loss        | 25.7     |
|    std                | 0.978    |
|    value_loss         | 293      |
------------------------------------
Eval num_timesteps=440000, episode_reward=146.63 +/- 3.46
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 147      |
| time/                 |          |
|    total_timesteps    | 440000   |
| train/                |          |
|    entropy_loss       | -3.07    |
|    explained_variance | 0.0424   |
|    learning_rate      | 0.0003   |
|    n_updates          | 381      |
|    policy_loss        | 33.8     |
|    std                | 0.977    |
|    value_loss         | 442      |
------------------------------------
Eval num_timesteps=460000, episode_reward=145.54 +/- 2.69
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 146      |
| time/                 |          |
|    total_timesteps    | 460000   |
| train/                |          |
|    entropy_loss       | -3.09    |
|    explained_variance | 0.395    |
|    learning_rate      | 0.0003   |
|    n_updates          | 399      |
|    policy_loss        | 35.8     |
|    std                | 0.975    |
|    value_loss         | 403      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 789      |
|    iterations      | 400      |
|    time_elapsed    | 583      |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=480000, episode_reward=144.55 +/- 2.03
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 145      |
| time/                 |          |
|    total_timesteps    | 480000   |
| train/                |          |
|    entropy_loss       | -3.11    |
|    explained_variance | 0.396    |
|    learning_rate      | 0.0003   |
|    n_updates          | 416      |
|    policy_loss        | 42.9     |
|    std                | 0.974    |
|    value_loss         | 432      |
------------------------------------
Eval num_timesteps=500000, episode_reward=145.55 +/- 3.56
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 146      |
| time/                 |          |
|    total_timesteps    | 500000   |
| train/                |          |
|    entropy_loss       | -2.98    |
|    explained_variance | -0.0745  |
|    learning_rate      | 0.0003   |
|    n_updates          | 434      |
|    policy_loss        | 48.9     |
|    std                | 0.972    |
|    value_loss         | 527      |
------------------------------------
Eval num_timesteps=520000, episode_reward=144.08 +/- 2.90
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 520000   |
| train/                |          |
|    entropy_loss       | -3.22    |
|    explained_variance | 0.392    |
|    learning_rate      | 0.0003   |
|    n_updates          | 451      |
|    policy_loss        | 22.5     |
|    std                | 0.972    |
|    value_loss         | 288      |
------------------------------------
Eval num_timesteps=540000, episode_reward=145.29 +/- 2.42
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 145      |
| time/                 |          |
|    total_timesteps    | 540000   |
| train/                |          |
|    entropy_loss       | -3.12    |
|    explained_variance | 0.298    |
|    learning_rate      | 0.0003   |
|    n_updates          | 468      |
|    policy_loss        | 33.3     |
|    std                | 0.969    |
|    value_loss         | 375      |
------------------------------------
Eval num_timesteps=560000, episode_reward=144.30 +/- 2.88
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 560000   |
| train/                |          |
|    entropy_loss       | -3.12    |
|    explained_variance | 0.424    |
|    learning_rate      | 0.0003   |
|    n_updates          | 486      |
|    policy_loss        | 33.4     |
|    std                | 0.967    |
|    value_loss         | 379      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 110      |
| time/                 |          |
|    fps                | 792      |
|    iterations         | 500      |
|    time_elapsed       | 727      |
|    total_timesteps    | 576000   |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | -0.0542  |
|    learning_rate      | 0.0003   |
|    n_updates          | 499      |
|    policy_loss        | 49.4     |
|    std                | 0.966    |
|    value_loss         | 490      |
------------------------------------
Eval num_timesteps=580000, episode_reward=144.84 +/- 3.05
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 145      |
| time/                 |          |
|    total_timesteps    | 580000   |
| train/                |          |
|    entropy_loss       | -3.23    |
|    explained_variance | 0.561    |
|    learning_rate      | 0.0003   |
|    n_updates          | 503      |
|    policy_loss        | 19.6     |
|    std                | 0.965    |
|    value_loss         | 220      |
------------------------------------
Eval num_timesteps=600000, episode_reward=144.31 +/- 2.08
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 600000   |
| train/                |          |
|    entropy_loss       | -3.03    |
|    explained_variance | -0.0484  |
|    learning_rate      | 0.0003   |
|    n_updates          | 520      |
|    policy_loss        | 47.7     |
|    std                | 0.963    |
|    value_loss         | 455      |
------------------------------------
Eval num_timesteps=620000, episode_reward=144.24 +/- 3.87
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 620000   |
| train/                |          |
|    entropy_loss       | -3.13    |
|    explained_variance | 0.332    |
|    learning_rate      | 0.0003   |
|    n_updates          | 538      |
|    policy_loss        | 28.1     |
|    std                | 0.961    |
|    value_loss         | 367      |
------------------------------------
Eval num_timesteps=640000, episode_reward=142.91 +/- 2.64
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 640000   |
| train/                |          |
|    entropy_loss       | -3.14    |
|    explained_variance | 0.114    |
|    learning_rate      | 0.0003   |
|    n_updates          | 555      |
|    policy_loss        | 25.7     |
|    std                | 0.959    |
|    value_loss         | 475      |
------------------------------------
Eval num_timesteps=660000, episode_reward=144.39 +/- 3.59
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 660000   |
| train/                |          |
|    entropy_loss       | -3.04    |
|    explained_variance | -0.0371  |
|    learning_rate      | 0.0003   |
|    n_updates          | 572      |
|    policy_loss        | 49.6     |
|    std                | 0.958    |
|    value_loss         | 415      |
------------------------------------
Eval num_timesteps=680000, episode_reward=142.43 +/- 2.58
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 680000   |
| train/                |          |
|    entropy_loss       | -3.19    |
|    explained_variance | 0.496    |
|    learning_rate      | 0.0003   |
|    n_updates          | 590      |
|    policy_loss        | 29.5     |
|    std                | 0.956    |
|    value_loss         | 290      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 128      |
| time/                 |          |
|    fps                | 790      |
|    iterations         | 600      |
|    time_elapsed       | 873      |
|    total_timesteps    | 691200   |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | -0.0334  |
|    learning_rate      | 0.0003   |
|    n_updates          | 599      |
|    policy_loss        | 38.2     |
|    std                | 0.956    |
|    value_loss         | 371      |
------------------------------------
Eval num_timesteps=700000, episode_reward=141.67 +/- 2.43
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 700000   |
| train/                |          |
|    entropy_loss       | -3.18    |
|    explained_variance | 0.5      |
|    learning_rate      | 0.0003   |
|    n_updates          | 607      |
|    policy_loss        | 38.7     |
|    std                | 0.956    |
|    value_loss         | 271      |
------------------------------------
Eval num_timesteps=720000, episode_reward=142.58 +/- 2.69
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 720000   |
| train/                |          |
|    entropy_loss       | -3.13    |
|    explained_variance | -0.00175 |
|    learning_rate      | 0.0003   |
|    n_updates          | 624      |
|    policy_loss        | 23.8     |
|    std                | 0.954    |
|    value_loss         | 326      |
------------------------------------
Eval num_timesteps=740000, episode_reward=142.80 +/- 2.47
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 740000   |
| train/                |          |
|    entropy_loss       | -3.19    |
|    explained_variance | 0.521    |
|    learning_rate      | 0.0003   |
|    n_updates          | 642      |
|    policy_loss        | 27.8     |
|    std                | 0.951    |
|    value_loss         | 256      |
------------------------------------
Eval num_timesteps=760000, episode_reward=143.94 +/- 3.79
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 760000   |
| train/                |          |
|    entropy_loss       | -3.09    |
|    explained_variance | -0.0216  |
|    learning_rate      | 0.0003   |
|    n_updates          | 659      |
|    policy_loss        | 44.4     |
|    std                | 0.949    |
|    value_loss         | 447      |
------------------------------------
Eval num_timesteps=780000, episode_reward=142.83 +/- 2.45
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 780000   |
| train/                |          |
|    entropy_loss       | -3.09    |
|    explained_variance | -0.0229  |
|    learning_rate      | 0.0003   |
|    n_updates          | 677      |
|    policy_loss        | 36.8     |
|    std                | 0.945    |
|    value_loss         | 320      |
------------------------------------
Eval num_timesteps=800000, episode_reward=142.62 +/- 2.75
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 800000   |
| train/                |          |
|    entropy_loss       | -3.12    |
|    explained_variance | -0.0208  |
|    learning_rate      | 0.0003   |
|    n_updates          | 694      |
|    policy_loss        | 36.3     |
|    std                | 0.943    |
|    value_loss         | 327      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 131      |
| time/                 |          |
|    fps                | 790      |
|    iterations         | 700      |
|    time_elapsed       | 1020     |
|    total_timesteps    | 806400   |
| train/                |          |
|    entropy_loss       | -3.26    |
|    explained_variance | 0.665    |
|    learning_rate      | 0.0003   |
|    n_updates          | 699      |
|    policy_loss        | 16.5     |
|    std                | 0.944    |
|    value_loss         | 151      |
------------------------------------
Eval num_timesteps=820000, episode_reward=142.73 +/- 3.59
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 820000   |
| train/                |          |
|    entropy_loss       | -3.09    |
|    explained_variance | -0.0182  |
|    learning_rate      | 0.0003   |
|    n_updates          | 711      |
|    policy_loss        | 35.4     |
|    std                | 0.942    |
|    value_loss         | 331      |
------------------------------------
Eval num_timesteps=840000, episode_reward=142.60 +/- 2.75
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 840000   |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | -0.0171  |
|    learning_rate      | 0.0003   |
|    n_updates          | 729      |
|    policy_loss        | 32.2     |
|    std                | 0.938    |
|    value_loss         | 314      |
------------------------------------
Eval num_timesteps=860000, episode_reward=143.71 +/- 3.38
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 144      |
| time/                 |          |
|    total_timesteps    | 860000   |
| train/                |          |
|    entropy_loss       | -3.19    |
|    explained_variance | 0.549    |
|    learning_rate      | 0.0003   |
|    n_updates          | 746      |
|    policy_loss        | 23.1     |
|    std                | 0.935    |
|    value_loss         | 213      |
------------------------------------
Eval num_timesteps=880000, episode_reward=142.82 +/- 3.01
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 880000   |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | -0.0153  |
|    learning_rate      | 0.0003   |
|    n_updates          | 763      |
|    policy_loss        | 30.8     |
|    std                | 0.935    |
|    value_loss         | 289      |
------------------------------------
Eval num_timesteps=900000, episode_reward=142.44 +/- 3.13
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 900000   |
| train/                |          |
|    entropy_loss       | -3.09    |
|    explained_variance | -0.014   |
|    learning_rate      | 0.0003   |
|    n_updates          | 781      |
|    policy_loss        | 33.5     |
|    std                | 0.934    |
|    value_loss         | 293      |
------------------------------------
Eval num_timesteps=920000, episode_reward=142.38 +/- 2.91
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 920000   |
| train/                |          |
|    entropy_loss       | -3.01    |
|    explained_variance | -0.0125  |
|    learning_rate      | 0.0003   |
|    n_updates          | 798      |
|    policy_loss        | 35       |
|    std                | 0.932    |
|    value_loss         | 317      |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 288      |
|    ep_rew_mean        | 130      |
| time/                 |          |
|    fps                | 790      |
|    iterations         | 800      |
|    time_elapsed       | 1166     |
|    total_timesteps    | 921600   |
| train/                |          |
|    entropy_loss       | -3.2     |
|    explained_variance | 0.558    |
|    learning_rate      | 0.0003   |
|    n_updates          | 799      |
|    policy_loss        | 22.9     |
|    std                | 0.932    |
|    value_loss         | 214      |
------------------------------------
Eval num_timesteps=940000, episode_reward=141.59 +/- 2.62
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 940000   |
| train/                |          |
|    entropy_loss       | -3.17    |
|    explained_variance | -0.0122  |
|    learning_rate      | 0.0003   |
|    n_updates          | 815      |
|    policy_loss        | 32       |
|    std                | 0.93     |
|    value_loss         | 295      |
------------------------------------
Eval num_timesteps=960000, episode_reward=143.41 +/- 2.85
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 960000   |
| train/                |          |
|    entropy_loss       | -3.12    |
|    explained_variance | -0.0111  |
|    learning_rate      | 0.0003   |
|    n_updates          | 833      |
|    policy_loss        | 26.3     |
|    std                | 0.929    |
|    value_loss         | 259      |
------------------------------------
Eval num_timesteps=980000, episode_reward=142.76 +/- 3.70
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 143      |
| time/                 |          |
|    total_timesteps    | 980000   |
| train/                |          |
|    entropy_loss       | -3.18    |
|    explained_variance | 0.586    |
|    learning_rate      | 0.0003   |
|    n_updates          | 850      |
|    policy_loss        | 20.9     |
|    std                | 0.927    |
|    value_loss         | 195      |
------------------------------------
Eval num_timesteps=1000000, episode_reward=141.77 +/- 2.95
Episode length: 288.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 288      |
|    mean_reward        | 142      |
| time/                 |          |
|    total_timesteps    | 1000000  |
| train/                |          |
|    entropy_loss       | -3.13    |
|    explained_variance | -0.00997 |
|    learning_rate      | 0.0003   |
|    n_updates          | 868      |
|    policy_loss        | 24.4     |
|    std                | 0.925    |
|    value_loss         | 246      |
------------------------------------
 100% ━━━━━━━━━━━━━━━━━━━━ 1,001,088/1,000,000  [ 0:21:05 < 0:00:00 , 731 it/s ]
Training completed successfully.

============================= JOB FEEDBACK =============================

Job ID: 1521760
Cluster: haic
User/Group: ii6824/iai
Account: iai
State: COMPLETED (exit code 0)
Partition: normal
Nodes: 1
Cores per node: 2
Nodelist: haicn1701
CPU Utilized: 00:36:15
CPU Efficiency: 83.65% of 00:43:20 core-walltime
Job Wall-clock time: 00:21:40
Starttime: Sun Jun  8 15:55:43 2025
Endtime: Sun Jun  8 16:17:23 2025
Memory Utilized: 2.67 GB
Memory Efficiency: 41.41% of 6.45 GB
Energy Consumed: 770990 Joule / 214.163888888889 Watthours
Average node power draw: 593.069230769231 Watt
